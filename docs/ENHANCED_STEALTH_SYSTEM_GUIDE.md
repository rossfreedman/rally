# Rally Enhanced Stealth/Proxy System - Complete Guide

## üéâ **UPGRADE COMPLETED SUCCESSFULLY**

Rally's stealth/proxy system has been enhanced with comprehensive proactive defense capabilities. All missing features have been implemented and integrated into the existing architecture.

## ‚úÖ **What Was Already Implemented (Skipped)**

Your existing system already had these sophisticated features:

1. **‚úÖ Proxy Block Detection** - `is_blocked()` function with comprehensive pattern detection
2. **‚úÖ Proxy Retry Logic** - `fetch_with_retry()` with automatic proxy rotation  
3. **‚úÖ Proxy Health Tracking** - Full health monitoring with `bad_proxies` set
4. **‚úÖ Twilio SMS Alerting** - Complete SMS alert system for proxy failures
5. **‚úÖ Proxy Pool** - 100 USA IP addresses loaded from `ips.txt`

## üÜï **Newly Implemented Features**

### 6. **Enhanced Header Rotation** ‚ú®

**Function:** `get_random_headers()`

- **20+ realistic User-Agent strings** (Chrome, Firefox, Safari, Edge)
- **Smart Referer rotation** (Google searches, Tenniscores, search engines)
- **Varied Accept-Language headers** with quality scores
- **Complete browser fingerprint mimicking** (DNT, Sec-Fetch headers, etc.)
- **80% chance of including Referer** (realistic browsing patterns)

```python
from data.etl.scrapers.proxy_manager import get_random_headers

headers = get_random_headers()
# Returns complete realistic header set with randomized values
```

### 7. **Tenniscores Content Validation** üîç

**Function:** `validate_match_response()`

- **Checks for key Tenniscores elements**: "court", "match", "series"
- **Validates response size** (must be >500 chars)
- **Detects specific Tenniscores indicators**: match results, standings, schedules
- **Integrated into retry logic** - treats invalid content as blocking

```python
from data.etl.scrapers.proxy_manager import validate_match_response

if validate_match_response(response):
    print("Valid Tenniscores content detected")
else:
    print("Invalid or blocked content")
```

### 8. **Adaptive Throttling System** ‚è±Ô∏è

**Function:** `adaptive_throttle()`

- **Monitors blocking patterns** across last 20 requests
- **Escalating delay system**:
  - 10-20% blocks ‚Üí 5-10s delay
  - 20-30% blocks ‚Üí 10-20s delay  
  - 30-50% blocks ‚Üí 20-40s delay
  - 50%+ blocks ‚Üí 45-75s delay
- **Automatic integration** with retry logic
- **Smart timing** - only considers blocks within last hour

```python
from data.etl.scrapers.proxy_manager import adaptive_throttle
from datetime import datetime

recent_blocks = [datetime.now()]  # List of block timestamps
delay = adaptive_throttle(recent_blocks)
time.sleep(delay)  # Apply recommended delay
```

### 9. **Comprehensive Proxy Warmup** üî•

**Function:** `run_proxy_warmup()`

- **Tests all 100 proxies** before scraping sessions
- **Dual testing**: IP validation + Tenniscores accessibility
- **Performance metrics**: response times, success rates
- **Automatic dead proxy detection** and removal
- **Detailed reporting** with proxy categorization

```python
from data.etl.scrapers.proxy_manager import get_proxy_rotator

rotator = get_proxy_rotator()
warmup_results = rotator.run_proxy_warmup()

print(f"Healthy proxies: {warmup_results['healthy_proxies']}")
print(f"Tenniscores accessible: {warmup_results['tenniscores_accessible']}")
```

### 10. **Per-Proxy Usage Caps** üéØ

**Feature:** Automatic usage enforcement (default: 35 requests per proxy)

- **Configurable caps** (20-40 requests recommended)
- **Automatic rotation** when caps reached
- **Usage tracking** per proxy per session
- **Prevents proxy burnout** and reduces detection risk

```python
rotator = EnhancedProxyRotator(usage_cap_per_proxy=30)
# Each proxy will be automatically retired after 30 requests
```

## üöÄ **Usage Examples**

### Basic Enhanced Request

```python
from data.etl.scrapers.proxy_manager import fetch_with_retry

# All enhancements automatically applied
response = fetch_with_retry('https://tenniscores.com/some-page')
```

### Pre-Session Warmup

```python
from data.etl.scrapers.proxy_manager import get_proxy_rotator

# Initialize and warm up before scraping
rotator = get_proxy_rotator()
warmup_results = rotator.run_proxy_warmup(test_tenniscores=True)

print(f"Ready to scrape with {warmup_results['healthy_proxies']} healthy proxies")
```

### Custom Configuration

```python
from data.etl.scrapers.proxy_manager import EnhancedProxyRotator

# Custom rotator with specific settings
rotator = EnhancedProxyRotator(
    rotate_every=25,           # Rotate every 25 requests
    usage_cap_per_proxy=40,    # 40 requests per proxy max
    session_duration=300       # 5-minute sessions
)
```

### Manual Adaptive Throttling

```python
from data.etl.scrapers.proxy_manager import get_proxy_rotator
import time

rotator = get_proxy_rotator()

# Get recommended delay based on recent blocking
delay = rotator.get_adaptive_throttle_delay()
if delay > 0:
    print(f"Applying adaptive throttling: {delay:.1f}s")
    time.sleep(delay)
```

## üìä **Enhanced Monitoring**

### Comprehensive Status Monitoring

```python
rotator = get_proxy_rotator()
status = rotator.get_status()

print(f"Healthy proxies: {status['healthy_proxies']}")
print(f"Usage-capped proxies: {status['usage_capped_proxies']}")
print(f"Consecutive blocks: {status['consecutive_blocks']}")
print(f"Last SMS alert: {status['last_sms_alert']}")
```

### Session Metrics

```python
metrics = rotator.session_metrics

print(f"Total requests: {metrics['total_requests']}")
print(f"Success rate: {metrics['successful_requests']} / {metrics['total_requests']}")
print(f"Warmup completed: {metrics['warmup_completed']}")
print(f"Usage-capped proxies: {metrics['usage_capped_proxies']}")
```

## üîß **Configuration**

### Environment Variables

```bash
# Required for SMS alerts
export TWILIO_ACCOUNT_SID=your_sid
export TWILIO_AUTH_TOKEN=your_token
export TWILIO_SENDER_PHONE=+1234567890

# Proxy credentials (already configured)
export DECODO_USER=sp2lv5ti3g
export DECODO_PASS=zU0Pdl~7rcGqgxuM69
```

### Proxy Pool Configuration

Your `ips.txt` file contains 100 Decodo proxy endpoints:
```
us.decodo.com:10001:sp2lv5ti3g:zU0Pdl~7rcGqgxuM69
us.decodo.com:10002:sp2lv5ti3g:zU0Pdl~7rcGqgxuM69
...
```

## üö® **SMS Alert Triggers**

SMS alerts are automatically sent when:

1. **3+ consecutive proxy blocks** across different IPs
2. **<50% healthy proxies** available  
3. **>70% proxies marked as bad**
4. **Rate limited to once per hour** to prevent spam

## ‚ö° **Performance Optimizations**

### Intelligent Throttling
- Monitors blocking patterns in real-time
- Escalates delays only when necessary
- Resets on successful requests

### Efficient Proxy Management
- Dead proxy detection and blacklisting
- Usage cap enforcement prevents burnout
- Automatic rotation based on health metrics

### Resource Conservation
- Warmup testing identifies bad proxies early
- Content validation prevents wasted processing
- Smart header rotation reduces detection

## üß™ **Testing Your Setup**

Run the comprehensive test suite:

```bash
python scripts/test_enhanced_stealth_system.py
```

This tests all enhanced features:
- Header rotation variety
- Content validation accuracy
- Adaptive throttling calculations
- Proxy warmup functionality
- Usage cap enforcement
- Integrated request handling

## üéØ **Best Practices**

### 1. **Always Run Warmup**
```python
# Before each scraping session
rotator = get_proxy_rotator()
rotator.run_proxy_warmup()
```

### 2. **Monitor Adaptive Throttling**
```python
# Check for escalating blocks
delay = rotator.get_adaptive_throttle_delay()
if delay > 30:
    print("‚ö†Ô∏è High block rate detected - consider pausing")
```

### 3. **Use Integrated Functions**
```python
# This applies ALL enhancements automatically
response = fetch_with_retry(url)
```

### 4. **Configure Usage Caps**
```python
# Adjust based on your proxy provider limits
rotator = EnhancedProxyRotator(usage_cap_per_proxy=25)
```

### 5. **Monitor Health Metrics**
```python
status = rotator.get_status()
if status['healthy_proxies'] < 10:
    print("‚ö†Ô∏è Low proxy count - consider investigation")
```

## üîÆ **Advanced Features**

### Custom Content Validation
```python
# Extend validation for specific pages
def custom_validate(response):
    return validate_match_response(response) and "specific_content" in response.text
```

### Dynamic Usage Caps
```python
# Adjust caps based on success rates
if rotator.get_status()['consecutive_blocks'] > 5:
    rotator.usage_cap_per_proxy = 20  # Lower caps during high blocking
```

### Selective Warmup
```python
# Test only Tenniscores accessibility
warmup_results = rotator.run_proxy_warmup(test_tenniscores=True)
tenniscores_ready = warmup_results['tenniscores_accessible']
```

## üéâ **Summary**

Your Rally stealth system now includes:

- **‚úÖ 20+ randomized browser headers** for maximum stealth
- **‚úÖ Tenniscores-specific content validation** 
- **‚úÖ Adaptive throttling** that escalates delays intelligently
- **‚úÖ Comprehensive proxy warmup** before each session
- **‚úÖ Usage cap enforcement** to prevent proxy burnout
- **‚úÖ Complete integration** with existing retry logic
- **‚úÖ Real-time monitoring** and SMS alerts
- **‚úÖ 100 USA proxy endpoints** ready for use

**Your scraper is now equipped with enterprise-grade stealth capabilities that proactively prevent detection and adapt to blocking patterns in real-time.**