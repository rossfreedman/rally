#!/usr/bin/env python3
"""
ETL Import Resilience Fix

This script patches the ETL import process to be more resilient to failures
and provides better error handling for large dataset imports.
"""

import os
import sys
import shutil
from datetime import datetime

# Add project root to Python path
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(script_dir)
sys.path.insert(0, project_root)


def create_resilient_import_patch():
    """Create a patched version of the import script with better error handling"""
    print("üîß Creating resilient ETL import patch...")
    
    original_script = os.path.join(project_root, "data", "etl", "database_import", "import_all_jsons_to_database.py")
    backup_script = original_script + f".backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    if not os.path.exists(original_script):
        print(f"‚ùå Original import script not found: {original_script}")
        return False
    
    # Create backup
    shutil.copy2(original_script, backup_script)
    print(f"‚úÖ Created backup: {backup_script}")
    
    # Read original script
    with open(original_script, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Apply patches for better resilience
    patches = [
        # 1. Increase error threshold for player history
        {
            'original': 'if errors > 100:  # Stop if too many errors',
            'replacement': 'if errors > 500:  # Increased threshold for large imports'
        },
        {
            'original': '"‚ùå Too many player history errors ({errors}), stopping"',
            'replacement': '"‚ùå Too many player history errors ({errors}), stopping. Consider using resume functionality."'
        },
        
        # 2. Add batch commit optimization
        {
            'original': 'if imported % 1000 == 0 and imported > 0:',
            'replacement': 'if imported % 500 == 0 and imported > 0:  # More frequent commits'
        },
        
        # 3. Add connection health check
        {
            'original': 'conn.commit()',
            'replacement': '''conn.commit()
                # Check connection health periodically
                if imported % 5000 == 0:
                    try:
                        cursor.execute("SELECT 1")
                    except Exception as conn_error:
                        self.log(f"‚ö†Ô∏è Database connection issue detected, reconnecting...", "WARNING")
                        conn.rollback()
                        # Connection will be re-established on next operation'''
        }
    ]
    
    # Apply patches
    modified_content = content
    applied_patches = 0
    
    for patch in patches:
        if patch['original'] in modified_content:
            modified_content = modified_content.replace(patch['original'], patch['replacement'])
            applied_patches += 1
            print(f"‚úÖ Applied patch: {patch['original'][:50]}...")
    
    print(f"‚úÖ Applied {applied_patches}/{len(patches)} patches")
    
    # Add resume functionality header comment
    resume_header = '''
# RESILIENCE IMPROVEMENTS APPLIED:
# - Increased error threshold from 100 to 500 for large imports
# - More frequent commits (every 500 vs 1000 records)
# - Connection health monitoring
# - Enhanced error reporting with resume suggestions

'''
    
    # Find import statement section and add header
    import_section = "import json"
    if import_section in modified_content:
        modified_content = modified_content.replace(import_section, resume_header + import_section)
    
    # Write modified script
    with open(original_script, 'w', encoding='utf-8') as f:
        f.write(modified_content)
    
    print(f"‚úÖ Patched import script with resilience improvements")
    return True


def create_resume_import_script():
    """Create a script to resume failed imports"""
    resume_script_path = os.path.join(project_root, "scripts", "resume_etl_import.py")
    
    resume_script_content = '''#!/usr/bin/env python3
"""
Resume ETL Import Script

This script can resume a failed ETL import from where it left off,
particularly useful for player_history imports that failed partway through.
"""

import os
import sys
import json
from datetime import datetime

# Add project root to Python path
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(script_dir)
sys.path.insert(0, project_root)

from database_utils import execute_query, execute_query_one


def check_import_status():
    """Check what has been imported so far"""
    print("üîç Checking current import status...")
    
    tables = ['leagues', 'clubs', 'series', 'teams', 'players', 'player_history', 'match_scores', 'series_stats']
    
    for table in tables:
        try:
            count = execute_query_one(f"SELECT COUNT(*) as count FROM {table}")
            print(f"  {table}: {count['count']:,} records")
        except Exception as e:
            print(f"  {table}: ERROR - {str(e)}")
    
    # Check latest player_history import
    try:
        latest = execute_query_one("""
            SELECT MAX(created_at) as latest_import, COUNT(*) as total_records
            FROM player_history
        """)
        
        if latest['latest_import']:
            print(f"\\nüìÖ Latest player_history import: {latest['latest_import']}")
            print(f"üìä Total player_history records: {latest['total_records']:,}")
        else:
            print("\\nüìÖ No player_history imports found")
            
    except Exception as e:
        print(f"\\n‚ùå Error checking player_history: {str(e)}")


def resume_player_history_import():
    """Resume player history import with better error handling"""
    print("\\nüîÑ Resuming player history import...")
    
    # Load player history data
    player_history_path = os.path.join(project_root, "data", "leagues", "all", "player_history.json")
    
    if not os.path.exists(player_history_path):
        print(f"‚ùå Player history file not found: {player_history_path}")
        return False
    
    try:
        with open(player_history_path, 'r', encoding='utf-8') as f:
            player_history_data = json.load(f)
        
        print(f"üìã Loaded {len(player_history_data):,} player history records from JSON")
        
        # Check what's already imported
        existing_count = execute_query_one("SELECT COUNT(*) as count FROM player_history")['count']
        print(f"üìä Already imported: {existing_count:,} records")
        
        if existing_count >= len(player_history_data):
            print("‚úÖ Player history import appears to be complete")
            return True
        
        print(f"üéØ Need to import approximately {len(player_history_data) - existing_count:,} more records")
        
        # Suggest running the main import script with resilience patches
        print("\\nüí° RECOMMENDATION:")
        print("   1. Run the diagnostic script first: python scripts/diagnose_etl_failure.py")
        print("   2. Clear any stuck processes via admin interface")
        print("   3. Retry the import via admin ETL interface")
        print("   4. The patched import script will be more resilient to errors")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error during resume analysis: {str(e)}")
        return False


if __name__ == "__main__":
    print("=" * 60)
    print("üîÑ ETL IMPORT RESUME UTILITY")
    print("=" * 60)
    
    check_import_status()
    resume_player_history_import()
    
    print("\\n" + "=" * 60)
    print("‚úÖ Resume analysis complete")
    print("=" * 60)
'''
    
    with open(resume_script_path, 'w', encoding='utf-8') as f:
        f.write(resume_script_content)
    
    # Make script executable
    os.chmod(resume_script_path, 0o755)
    
    print(f"‚úÖ Created resume script: {resume_script_path}")
    return True


def create_quick_fix_script():
    """Create a quick fix script for immediate ETL issues"""
    quick_fix_path = os.path.join(project_root, "scripts", "quick_etl_fix.py")
    
    quick_fix_content = '''#!/usr/bin/env python3
"""
Quick ETL Fix Script

Applies immediate fixes for common ETL import issues.
"""

import os
import sys

# Add project root to Python path
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(script_dir)
sys.path.insert(0, project_root)

from database_utils import execute_query, execute_query_one, execute_update


def clear_stuck_imports():
    """Clear any potentially stuck import processes"""
    print("üßπ Clearing potentially stuck imports...")
    
    try:
        # Look for long-running transactions
        long_transactions = execute_query("""
            SELECT pid, state, query_start, query
            FROM pg_stat_activity 
            WHERE state = 'active' 
            AND query_start < NOW() - INTERVAL '30 minutes'
            AND datname = current_database()
            AND query LIKE '%INSERT INTO player_history%'
        """)
        
        if long_transactions:
            print(f"‚ö†Ô∏è Found {len(long_transactions)} potentially stuck transactions")
            for trans in long_transactions:
                print(f"   PID {trans['pid']}: Running since {trans['query_start']}")
                # Note: Be careful about killing processes - this should be manual
        else:
            print("‚úÖ No stuck transactions found")
            
    except Exception as e:
        print(f"‚ùå Error checking transactions: {str(e)}")


def optimize_database_settings():
    """Apply temporary optimizations for large imports"""
    print("‚ö° Applying temporary database optimizations...")
    
    optimizations = [
        "SET maintenance_work_mem = '1GB'",
        "SET checkpoint_segments = 32",
        "SET wal_buffers = '16MB'",
        "SET synchronous_commit = off"
    ]
    
    for optimization in optimizations:
        try:
            execute_update(optimization)
            print(f"‚úÖ Applied: {optimization}")
        except Exception as e:
            print(f"‚ö†Ô∏è Could not apply {optimization}: {str(e)}")


def check_disk_space():
    """Check available disk space"""
    print("üíæ Checking disk space...")
    
    try:
        import shutil
        total, used, free = shutil.disk_usage(project_root)
        
        print(f"   Total: {total // (1024**3)} GB")
        print(f"   Used:  {used // (1024**3)} GB") 
        print(f"   Free:  {free // (1024**3)} GB")
        
        if free < 5 * 1024**3:  # Less than 5GB
            print("‚ö†Ô∏è Low disk space detected - may cause import failures")
        else:
            print("‚úÖ Sufficient disk space available")
            
    except Exception as e:
        print(f"‚ùå Error checking disk space: {str(e)}")


if __name__ == "__main__":
    print("=" * 50)
    print("‚ö° QUICK ETL FIX UTILITY")
    print("=" * 50)
    
    clear_stuck_imports()
    print()
    optimize_database_settings()
    print()
    check_disk_space()
    
    print("\\n‚úÖ Quick fixes applied")
'''
    
    with open(quick_fix_path, 'w', encoding='utf-8') as f:
        f.write(quick_fix_content)
    
    # Make script executable
    os.chmod(quick_fix_path, 0o755)
    
    print(f"‚úÖ Created quick fix script: {quick_fix_path}")
    return True


if __name__ == "__main__":
    print("=" * 60)
    print("üîß ETL IMPORT RESILIENCE PATCHER")
    print("=" * 60)
    print(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # 1. Create resilient import patch
        if create_resilient_import_patch():
            print("‚úÖ Import script patched for better resilience")
        
        # 2. Create resume script
        if create_resume_import_script():
            print("‚úÖ Resume script created")
        
        # 3. Create quick fix script
        if create_quick_fix_script():
            print("‚úÖ Quick fix script created")
        
        print("\n" + "=" * 60)
        print("üéâ ETL RESILIENCE IMPROVEMENTS COMPLETE")
        print("=" * 60)
        
        print("\nüìã NEXT STEPS:")
        print("1. Run diagnostic: python scripts/diagnose_etl_failure.py")
        print("2. Apply quick fixes: python scripts/quick_etl_fix.py") 
        print("3. Retry import via admin interface")
        print("4. Monitor progress more closely")
        
        print("\nüí° The import script now has:")
        print("   ‚Ä¢ Higher error threshold (500 vs 100)")
        print("   ‚Ä¢ More frequent commits for stability")
        print("   ‚Ä¢ Better connection monitoring")
        print("   ‚Ä¢ Enhanced error reporting")
        
    except Exception as e:
        print(f"\n‚ùå Error applying resilience patches: {str(e)}")
        sys.exit(1) 