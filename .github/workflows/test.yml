name: Rally Test Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.9'
  NODE_VERSION: '16'

jobs:
  # Job 1: Unit and Integration Tests
  test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_USER: postgres
          POSTGRES_DB: rally_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Cache Python dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y postgresql-client

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt

    - name: Install Chrome for Selenium tests
      uses: browser-actions/setup-chrome@latest

    - name: Set up test environment variables
      run: |
        echo "TEST_DATABASE_URL=postgresql://postgres:postgres@localhost:5432/rally_test" >> $GITHUB_ENV
        echo "FLASK_ENV=testing" >> $GITHUB_ENV
        echo "SECRET_KEY=test-secret-key-for-ci" >> $GITHUB_ENV

    - name: Wait for PostgreSQL
      run: |
        until pg_isready -h localhost -p 5432 -U postgres; do
          echo "Waiting for PostgreSQL..."
          sleep 2
        done

    - name: Run database migrations
      run: |
        python -c "
        from database_config import get_db_url
        from sqlalchemy import create_engine
        from app.models.database_models import Base
        engine = create_engine('${{ env.TEST_DATABASE_URL }}')
        Base.metadata.create_all(engine)
        print('Test database setup complete')
        "

    - name: Lint with flake8
      run: |
        # Stop the build if there are Python syntax errors or undefined names
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # Exit-zero treats all errors as warnings
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

    - name: Check code formatting with black
      run: |
        black --check --diff .

    - name: Check import sorting with isort
      run: |
        isort --check-only --diff .

    - name: Type checking with mypy (optional)
      continue-on-error: true
      run: |
        mypy . --ignore-missing-imports || echo "MyPy check completed with warnings"

    - name: Run unit tests
      run: |
        pytest tests/ -v \
          --cov=app \
          --cov=utils \
          --cov-report=xml \
          --cov-report=html \
          --cov-report=term-missing \
          --junit-xml=test-results.xml \
          -m "unit" \
          --tb=short

    - name: Run integration tests
      run: |
        pytest tests/ -v \
          --cov=app \
          --cov-append \
          --cov-report=xml \
          --junit-xml=integration-results.xml \
          -m "integration" \
          --tb=short

    - name: Run security tests
      run: |
        pytest tests/ -v \
          --junit-xml=security-results.xml \
          -m "security" \
          --tb=short

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results
        path: |
          test-results.xml
          integration-results.xml
          security-results.xml
          htmlcov/

    - name: Upload coverage report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: coverage-report
        path: htmlcov/

  # Job 2: UI Tests with Playwright (temporarily disabled)
  ui-tests:
    runs-on: ubuntu-latest
    needs: test
    if: false  # Skip UI tests for now until playwright is properly configured
    
    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_USER: postgres
          POSTGRES_DB: rally_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt

    - name: Install Playwright browsers
      run: |
        playwright install --with-deps

    - name: Set up test environment
      run: |
        echo "TEST_DATABASE_URL=postgresql://postgres:postgres@localhost:5432/rally_test" >> $GITHUB_ENV
        echo "FLASK_ENV=testing" >> $GITHUB_ENV
        echo "HEADLESS=true" >> $GITHUB_ENV

    - name: Wait for PostgreSQL
      run: |
        until pg_isready -h localhost -p 5432 -U postgres; do
          echo "Waiting for PostgreSQL..."
          sleep 2
        done

    - name: Setup test database
      run: |
        python -c "
        from database_config import get_db_url
        from sqlalchemy import create_engine
        from app.models.database_models import Base
        engine = create_engine('${{ env.TEST_DATABASE_URL }}')
        Base.metadata.create_all(engine)
        "

    - name: Run UI tests
      run: |
        python run_ui_tests.py --all --browser chromium --html-report ui_test_report.html --junit-xml ui_test_results.xml

    - name: Upload UI test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: ui-test-results
        path: |
          ui_test_report.html
          ui_test_results.xml
          ui_tests/screenshots/

    - name: Upload UI test screenshots
      uses: actions/upload-artifact@v3
      if: failure()
      with:
        name: ui-test-screenshots
        path: ui_tests/screenshots/

  # Job 3: Performance and Load Tests
  performance:
    runs-on: ubuntu-latest
    needs: test
    
    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_USER: postgres
          POSTGRES_DB: rally_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt

    - name: Set up test environment
      run: |
        echo "TEST_DATABASE_URL=postgresql://postgres:postgres@localhost:5432/rally_test" >> $GITHUB_ENV
        echo "FLASK_ENV=testing" >> $GITHUB_ENV

    - name: Wait for PostgreSQL
      run: |
        until pg_isready -h localhost -p 5432 -U postgres; do
          echo "Waiting for PostgreSQL..."
          sleep 2
        done

    - name: Setup test database
      run: |
        python -c "
        from database_config import get_db_url
        from sqlalchemy import create_engine
        from app.models.database_models import Base
        engine = create_engine('${{ env.TEST_DATABASE_URL }}')
        Base.metadata.create_all(engine)
        "

    - name: Run performance tests
      run: |
        pytest tests/ -v \
          --junit-xml=performance-results.xml \
          -m "performance" \
          --tb=short

    - name: Start Flask app for basic health check
      run: |
        export DATABASE_URL=${{ env.TEST_DATABASE_URL }}
        python server.py &
        sleep 10  # Wait for app to start
        
        # Verify app is running
        curl -f http://localhost:8080/health || exit 1
      timeout-minutes: 2

    - name: Basic load test simulation
      run: |
        # Simple load test using curl
        echo "Running basic load simulation..."
        for i in {1..10}; do
          curl -s http://localhost:8080/health > /dev/null
          echo "Request $i completed"
        done
        echo "Basic load test completed"

    - name: Upload performance results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-results
        path: |
          performance-results.xml
          tests/load/load_test_report.html
          tests/load/load_test_results*.csv

  # Job 3: Security Scan
  security:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt

    - name: Run Bandit security scan
      run: |
        bandit -r . -f json -o bandit-report.json || true
        bandit -r . -f txt

    - name: Check for known vulnerabilities with Safety
      run: |
        safety check --json --output safety-report.json || true
        safety check

    - name: Audit Python packages
      run: |
        pip-audit --format=json --output=pip-audit-report.json || true
        pip-audit

    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json
          pip-audit-report.json

  # Job 4: Regression Tests
  regression:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'push'
    
    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_USER: postgres
          POSTGRES_DB: rally_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt

    - name: Set up test environment
      run: |
        echo "TEST_DATABASE_URL=postgresql://postgres:postgres@localhost:5432/rally_test" >> $GITHUB_ENV
        echo "FLASK_ENV=testing" >> $GITHUB_ENV

    - name: Wait for PostgreSQL
      run: |
        until pg_isready -h localhost -p 5432 -U postgres; do
          echo "Waiting for PostgreSQL..."
          sleep 2
        done

    - name: Setup test database
      run: |
        python -c "
        from database_config import get_db_url
        from sqlalchemy import create_engine
        from app.models.database_models import Base
        engine = create_engine('${{ env.TEST_DATABASE_URL }}')
        Base.metadata.create_all(engine)
        "

    - name: Scrape fresh test data
      run: |
        python tests/scrapers/random_league_scraper.py || echo "Scraper completed with warnings"

    - name: Run full regression test suite
      run: |
        pytest tests/ -v \
          --junit-xml=regression-results.xml \
          -m "regression" \
          --tb=short

    - name: Run all tests as regression check
      run: |
        pytest tests/ -v \
          --junit-xml=full-regression-results.xml \
          --tb=short \
          --maxfail=5

    - name: Upload regression results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: regression-results
        path: |
          regression-results.xml
          full-regression-results.xml
          tests/fixtures/scraped_players*.json

  # Job 5: Build and Test Report
  report:
    runs-on: ubuntu-latest
    needs: [test, security, regression]
    if: always()

    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v3

    - name: Generate test report
      run: |
        echo "# Rally Test Suite Results" > test_report.md
        echo "" >> test_report.md
        echo "## Test Execution Summary" >> test_report.md
        echo "- **Date**: $(date)" >> test_report.md
        echo "- **Commit**: ${{ github.sha }}" >> test_report.md
        echo "- **Branch**: ${{ github.ref_name }}" >> test_report.md
        echo "" >> test_report.md
        
        # Check if test artifacts exist and report status
        if [ -d "test-results" ]; then
          echo "âœ… Unit and Integration Tests: COMPLETED" >> test_report.md
        else
          echo "âŒ Unit and Integration Tests: FAILED" >> test_report.md
        fi
        
        if [ -d "performance-results" ]; then
          echo "âœ… Performance Tests: COMPLETED" >> test_report.md
        else
          echo "âŒ Performance Tests: FAILED" >> test_report.md
        fi
        
        if [ -d "security-reports" ]; then
          echo "âœ… Security Scan: COMPLETED" >> test_report.md
        else
          echo "âŒ Security Scan: FAILED" >> test_report.md
        fi
        
        if [ -d "regression-results" ]; then
          echo "âœ… Regression Tests: COMPLETED" >> test_report.md
        else
          echo "âŒ Regression Tests: FAILED or SKIPPED" >> test_report.md
        fi
        
        echo "" >> test_report.md
        echo "## Next Steps" >> test_report.md
        echo "- Review coverage report in artifacts" >> test_report.md
        echo "- Check security scan results" >> test_report.md
        echo "- Examine load test performance metrics" >> test_report.md
        echo "- Address any failing tests before deployment" >> test_report.md

    - name: Upload test report
      uses: actions/upload-artifact@v3
      with:
        name: test-report
        path: test_report.md

    - name: Comment PR with test results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          try {
            const report = fs.readFileSync('test_report.md', 'utf8');
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: '## ðŸ§ª Test Suite Results\n\n' + report
            });
          } catch (error) {
            console.log('Could not read test report:', error);
          }

  # Job 6: Deployment Readiness Check
  deployment-check:
    runs-on: ubuntu-latest
    needs: [test, security]
    if: github.ref == 'refs/heads/main'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Check deployment readiness
      run: |
        echo "ðŸš€ Deployment Readiness Check"
        echo "âœ… All tests passed"
        echo "âœ… Security scan completed"
        echo "âœ… Code quality checks passed"
        echo ""
        echo "Ready for deployment to staging/production"

    - name: Trigger deployment
      if: success()
      run: |
        # This is where you would trigger your deployment pipeline
        # For example, calling Railway deployment API or other CD pipeline
        echo "Triggering deployment pipeline..."
        echo "Deployment would be triggered here in a real scenario" 